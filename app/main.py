import os
import uvicorn
from contextlib import asynccontextmanager
from typing import List, Optional, Dict, AsyncGenerator
from uuid import uuid4
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
load_dotenv()

from app.agent import build_agent_app, agent_app as fallback_agent_app

def _normalize_backend(raw: Optional[str]) -> str:
    if not raw:
        return "none"
    return raw.strip().lower()


async def init_checkpointer():
    backend = _normalize_backend(os.getenv("CHECKPOINT_BACKEND"))
    if backend in {"none", "off", "disabled"}:
        return None, None

    try:
        if backend == "sqlite":
            from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
            conn_str = os.getenv("CHECKPOINT_DB_URI", "checkpoints.sqlite")
            cm = AsyncSqliteSaver.from_conn_string(conn_str)
            saver = await cm.__aenter__()
            return saver, cm

        if backend == "postgres":
            try:
                from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
            except Exception as exc:
                raise RuntimeError("langgraph-checkpoint-postgres is not installed.") from exc

            conn_str = os.getenv("CHECKPOINT_DB_URI")
            if not conn_str:
                raise RuntimeError("CHECKPOINT_DB_URI is required for postgres backend.")

            cm = AsyncPostgresSaver.from_conn_string(conn_str)
            saver = await cm.__aenter__()
            await saver.setup()
            return saver, cm

        raise RuntimeError(f"Unsupported CHECKPOINT_BACKEND '{backend}'.")
    except Exception as exc:
        print(f"Checkpointing disabled: {exc}")
        return None, None


@asynccontextmanager
async def lifespan(app: FastAPI):
    checkpointer, cm = await init_checkpointer()
    app.state.agent_app = build_agent_app(checkpointer=checkpointer)
    app.state.checkpointer_cm = cm
    try:
        yield
    finally:
        if cm:
            await cm.__aexit__(None, None, None)

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ---
app = FastAPI(
    title="Nabd AI Platform",
    description="Ù…Ù†ØµØ© Ù†Ø¨Ø¶ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ù„",
    version="2.0.0",
    lifespan=lifespan,
)

def _parse_origins(raw: str) -> List[str]:
    return [origin.strip() for origin in raw.split(",") if origin.strip()]


def get_allowed_origins() -> List[str]:
    raw = os.getenv("CORS_ALLOW_ORIGINS")
    if raw:
        return _parse_origins(raw)

    origins: List[str] = []
    wasp_web_url = os.getenv("WASP_WEB_CLIENT_URL")
    if wasp_web_url:
        origins.append(wasp_web_url.strip())

    env = (os.getenv("ENV") or "development").lower()
    if env != "production":
        origins.extend(_parse_origins(os.getenv("CORS_DEV_ORIGINS", "http://localhost:3000")))

    # Remove duplicates while preserving order
    seen = set()
    deduped: List[str] = []
    for origin in origins:
        if origin not in seen:
            seen.add(origin)
            deduped.append(origin)
    return deduped


# ØªÙØ¹ÙŠÙ„ CORS Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ø¨Ø§Ù„Ø§ØªØµØ§Ù„
allowed_origins = get_allowed_origins()
app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Pydantic Models) ---
class ChatRequest(BaseModel):
    message: str
    mode: str = "general"  # general, coder, writer, researcher
    history: List[Dict[str, str]] = Field(default_factory=list)  # Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚
    stream: bool = False
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    tool_usage: Optional[List[str]] = None
    session_id: Optional[str] = None

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© (Groq) ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    raise ValueError("âš ï¸ GROQ_API_KEY is missing in .env file!")

# Ù†Ø³ØªØ®Ø¯Ù… Llama 3 Ù„Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„ÙƒÙØ§Ø¡Ø©
llm = ChatGroq(
    temperature=0.5,
    model_name="llama3-70b-8192", 
    api_key=GROQ_API_KEY
)

# --- ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª (System Prompts) ---
SYSTEM_PROMPTS = {
    "general": """Ø£Ù†Øª (Ù†Ø¨Ø¶)ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ·ÙˆØ± Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨. 
    Ù…Ù‡Ù…ØªÙƒ: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ÙˆØ¶ÙˆØ­ØŒ Ø¯Ù‚Ø©ØŒ ÙˆÙˆØ¯ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¯Ø§Ø¦Ù…Ù‹Ø§.""",
    
    "coder": """Ø£Ù†Øª Ù…Ø¨Ø±Ù…Ø¬ Ø®Ø¨ÙŠØ± ÙÙŠ Ù…Ù†ØµØ© Ù†Ø¨Ø¶.
    Ù…Ù‡Ù…ØªÙƒ: ÙƒØªØ§Ø¨Ø© Ø£ÙƒÙˆØ§Ø¯ Ù†Ø¸ÙŠÙØ© (Clean Code) ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ©.
    Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
    1. Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¨Ù„Ø§Ù‹ Ù„Ù„ØªÙ†ÙÙŠØ°.
    2. Ø§Ø´Ø±Ø­ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø¨Ø§Ø®ØªØµØ§Ø± Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙˆØ§ÙƒØªØ¨ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©.
    3. Ø§ØªØ¨Ø¹ Ù…Ø¹Ø§ÙŠÙŠØ± PEP8 ÙÙŠ Ø¨Ø§ÙŠØ«ÙˆÙ†.""",
    
    "writer": """Ø£Ù†Øª ÙƒØ§ØªØ¨ Ù…Ø¨Ø¯Ø¹ ÙˆÙ…Ø­ØªØ±Ù.
    Ù…Ù‡Ù…ØªÙƒ: ØµÙŠØ§ØºØ© Ù…Ø­ØªÙˆÙ‰ Ø¬Ø°Ø§Ø¨ØŒ Ø®Ø§Ù„ÙŠ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ØŒ ÙˆÙ…Ù†Ø³Ù‚ Ø¨Ø¹Ù†Ø§ÙŠØ©.
    Ø§Ø³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown Ù„Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ù‚ÙˆØ§Ø¦Ù….""",
    
    "researcher": """Ø£Ù†Øª Ø¨Ø§Ø­Ø« Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø¯Ù‚ÙŠÙ‚.
    Ù…Ù‡Ù…ØªÙƒ: ØªÙ‚Ø¯ÙŠÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ«Ù‚Ø©ØŒ ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ØŒ ÙˆØ°ÙƒØ± Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¥Ù† Ø£Ù…ÙƒÙ†.
    ØªØ¬Ù†Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø³Ø·Ø­ÙŠØ©."""
}

ARABIC_ENFORCEMENT = "ØªÙ†Ø¨ÙŠÙ‡ ØµØ§Ø±Ù…: ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø±Ø¯Ùƒ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ (Ø£Ùˆ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)ØŒ ÙˆØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ RTL."


def _resolve_session_id(request: ChatRequest) -> str:
    return request.session_id or str(uuid4())


def _get_agent_app():
    return getattr(app.state, "agent_app", fallback_agent_app)


def _invoke_config(session_id: str) -> Dict[str, Dict[str, str]]:
    return {"configurable": {"thread_id": session_id}}

def build_messages(request: ChatRequest) -> List[BaseMessage]:
    selected_system_prompt = SYSTEM_PROMPTS.get(request.mode, SYSTEM_PROMPTS["general"])
    full_system_message = f"{selected_system_prompt}\n\n{ARABIC_ENFORCEMENT}"

    messages: List[BaseMessage] = [SystemMessage(content=full_system_message)]

    for msg in request.history:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        else:
            messages.append(AIMessage(content=msg["content"]))

    messages.append(HumanMessage(content=request.message))
    return messages


def _format_sse(data: str, event: Optional[str] = None) -> str:
    lines = data.splitlines() or [""]
    payload = []
    if event:
        payload.append(f"event: {event}")
    payload.extend([f"data: {line}" for line in lines])
    return "\n".join(payload) + "\n\n"


def _chunk_text(text: str, chunk_size: int = 8) -> List[str]:
    words = text.split()
    if not words:
        return [""]
    chunks = []
    for idx in range(0, len(words), chunk_size):
        chunks.append(" ".join(words[idx:idx + chunk_size]) + " ")
    return chunks


async def process_chat(request: ChatRequest, session_id: str) -> str:
    messages = build_messages(request)
    agent_app = _get_agent_app()

    try:
        result = await agent_app.ainvoke({"messages": messages}, config=_invoke_config(session_id))
        last_message = result["messages"][-1]
        return last_message.content
    except Exception as e:
        print(f"Error: {str(e)}") # Ù„Ù„ØªØ´Ø®ÙŠØµ ÙÙŠ Ø§Ù„ØªÙŠØ±Ù…ÙŠÙ†Ø§Ù„
        return "Ø¹Ø°Ø±Ø§Ù‹ØŒ ÙˆØ§Ø¬Ù‡Øª Ù…Ø´ÙƒÙ„Ø© ØªÙ‚Ù†ÙŠØ© Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."


async def stream_chat(request: ChatRequest, session_id: str) -> AsyncGenerator[str, None]:
    messages = build_messages(request)
    agent_app = _get_agent_app()

    yield _format_sse(session_id, event="session")

    if not hasattr(agent_app, "astream_events"):
        full_text = await process_chat(request, session_id)
        for chunk in _chunk_text(full_text):
            yield _format_sse(chunk)
        yield _format_sse("[DONE]")
        return

    try:
        async for event in agent_app.astream_events(
            {"messages": messages},
            config=_invoke_config(session_id),
            version="v1",
        ):
            if event.get("event") != "on_chat_model_stream":
                continue

            chunk = event.get("data", {}).get("chunk")
            text = getattr(chunk, "content", None)
            if text:
                yield _format_sse(text)

        yield _format_sse("[DONE]")
    except Exception as e:
        yield _format_sse(f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø«: {str(e)}", event="error")

# --- Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoints) ---

@app.get("/")
async def root():
    return {"status": "online", "message": "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…Ù†ØµØ© Ù†Ø¨Ø¶ 2.0 ğŸš€"}

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "model": "llama3-70b-8192"}

@app.post("/run", response_model=ChatResponse)
async def run_agent(request: ChatRequest):
    """
    Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
    ØªØ³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ù„ÙˆØ¶Ø¹ (Mode) ÙˆØªØ¹ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø°ÙƒÙŠ.
    """
    if request.stream:
        session_id = _resolve_session_id(request)
        return StreamingResponse(
            stream_chat(request, session_id),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no"
            }
        )
    session_id = _resolve_session_id(request)
    ai_reply = await process_chat(request, session_id)
    return ChatResponse(response=ai_reply, session_id=session_id)


@app.post("/run/stream")
async def run_agent_stream(request: ChatRequest):
    """
    Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± (Streaming) Ø¹Ù„Ù‰ Ø´ÙƒÙ„ SSE.
    """
    session_id = _resolve_session_id(request)
    return StreamingResponse(
        stream_chat(request, session_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"
        }
    )

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… (Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±) ---
if __name__ == "__main__":
    uvicorn.run("app.main:app", host="0.0.0.0", port=5000, reload=True)
