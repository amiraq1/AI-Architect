import os
import uvicorn
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# ┘Е┘Г╪к╪и╪з╪к ╪з┘Д╪░┘Г╪з╪б ╪з┘Д╪з╪╡╪╖┘Ж╪з╪╣┘К
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# ╪к╪н┘Е┘К┘Д ┘Е╪к╪║┘К╪▒╪з╪к ╪з┘Д╪и┘К╪ж╪й
load_dotenv()

# --- ╪е╪╣╪п╪з╪п╪з╪к ╪з┘Д╪к╪╖╪и┘К┘В ---
app = FastAPI(
    title="Nabd AI Platform",
    description="┘Е┘Ж╪╡╪й ┘Ж╪и╪╢ ┘Д┘Д╪░┘Г╪з╪б ╪з┘Д╪з╪╡╪╖┘Ж╪з╪╣┘К ╪з┘Д┘Е╪│╪к┘В┘Д",
    version="2.0.0"
)

# ╪к┘Б╪╣┘К┘Д CORS ┘Д┘Д╪│┘Е╪з╪н ┘Д┘Д┘И╪з╪м┘З╪й ╪з┘Д╪г┘Е╪з┘Е┘К╪й ╪и╪з┘Д╪з╪к╪╡╪з┘Д
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ┘К┘Е┘Г┘Ж ╪к┘В┘К┘К╪п ┘З╪░╪з ┘Б┘К ╪з┘Д╪е┘Ж╪к╪з╪м
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ┘Ж┘Е╪з╪░╪м ╪з┘Д╪и┘К╪з┘Ж╪з╪к (Pydantic Models) ---
class ChatRequest(BaseModel):
    message: str
    mode: str = "general"  # general, coder, writer, researcher
    history: List[Dict[str, str]] = []  # ╪│╪м┘Д ╪з┘Д┘Е╪н╪з╪п╪л╪й ╪з┘Д╪│╪з╪и┘В

class ChatResponse(BaseModel):
    response: str
    tool_usage: Optional[List[str]] = None

# --- ╪е╪╣╪п╪з╪п ┘Ж┘Е┘И╪░╪м ╪з┘Д┘Д╪║╪й (Groq) ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    raise ValueError("тЪая╕П GROQ_API_KEY is missing in .env file!")

# ┘Ж╪│╪к╪о╪п┘Е Llama 3 ┘Д┘Д╪│╪▒╪╣╪й ┘И╪з┘Д┘Г┘Б╪з╪б╪й
llm = ChatGroq(
    temperature=0.5,
    model_name="llama3-70b-8192", 
    api_key=GROQ_API_KEY
)

# --- ╪к╪╣╪▒┘К┘Б ╪з┘Д╪и╪▒┘И┘Е╪и╪к╪з╪к (System Prompts) ---
SYSTEM_PROMPTS = {
    "general": """╪г┘Ж╪к (┘Ж╪и╪╢)╪М ┘Е╪│╪з╪╣╪п ╪░┘Г┘К ┘Е╪к╪╖┘И╪▒ ┘Д┘Д┘Е╪│╪к╪о╪п┘Е┘К┘Ж ╪з┘Д╪╣╪▒╪и. 
    ┘Е┘З┘Е╪к┘Г: ╪з┘Д╪е╪м╪з╪и╪й ╪и┘И╪╢┘И╪н╪М ╪п┘В╪й╪М ┘И┘И╪п┘К╪й. ╪з╪│╪к╪о╪п┘Е ╪з┘Д┘Д╪║╪й ╪з┘Д╪╣╪▒╪и┘К╪й ╪п╪з╪ж┘Е┘Л╪з.""",
    
    "coder": """╪г┘Ж╪к ┘Е╪и╪▒┘Е╪м ╪о╪и┘К╪▒ ┘Б┘К ┘Е┘Ж╪╡╪й ┘Ж╪и╪╢.
    ┘Е┘З┘Е╪к┘Г: ┘Г╪к╪з╪и╪й ╪г┘Г┘И╪з╪п ┘Ж╪╕┘К┘Б╪й (Clean Code) ┘И╪з╪н╪к╪▒╪з┘Б┘К╪й.
    ╪з┘Д┘В┘И╪з╪╣╪п:
    1. ╪з┘Д┘Г┘И╪п ┘К╪м╪и ╪г┘Ж ┘К┘Г┘И┘Ж ┘В╪з╪и┘Д╪з┘Л ┘Д┘Д╪к┘Ж┘Б┘К╪░.
    2. ╪з╪┤╪▒╪н ╪з┘Д┘Е┘Ж╪╖┘В ╪и╪з╪о╪к╪╡╪з╪▒ ╪и╪з┘Д╪╣╪▒╪и┘К╪й╪М ┘И╪з┘Г╪к╪и ╪з┘Д┘Г┘И╪п ╪и╪з┘Д╪е┘Ж╪м┘Д┘К╪▓┘К╪й.
    3. ╪з╪к╪и╪╣ ┘Е╪╣╪з┘К┘К╪▒ PEP8 ┘Б┘К ╪и╪з┘К╪л┘И┘Ж.""",
    
    "writer": """╪г┘Ж╪к ┘Г╪з╪к╪и ┘Е╪и╪п╪╣ ┘И┘Е╪н╪к╪▒┘Б.
    ┘Е┘З┘Е╪к┘Г: ╪╡┘К╪з╪║╪й ┘Е╪н╪к┘И┘Й ╪м╪░╪з╪и╪М ╪о╪з┘Д┘К ┘Е┘Ж ╪з┘Д╪г╪о╪╖╪з╪б╪М ┘И┘Е┘Ж╪│┘В ╪и╪╣┘Ж╪з┘К╪й.
    ╪з╪│╪к╪о╪п┘Е ╪к┘Ж╪│┘К┘В Markdown ┘Д┘Д╪╣┘Ж╪з┘И┘К┘Ж ┘И╪з┘Д┘В┘И╪з╪ж┘Е.""",
    
    "researcher": """╪г┘Ж╪к ╪и╪з╪н╪л ╪г┘Г╪з╪п┘К┘Е┘К ╪п┘В┘К┘В.
    ┘Е┘З┘Е╪к┘Г: ╪к┘В╪п┘К┘Е ┘Е╪╣┘Д┘И┘Е╪з╪к ┘Е┘И╪л┘В╪й╪М ╪к╪н┘Д┘К┘Д ╪╣┘Е┘К┘В╪М ┘И╪░┘Г╪▒ ╪з┘Д┘Е╪╡╪з╪п╪▒ ╪е┘Ж ╪г┘Е┘Г┘Ж.
    ╪к╪м┘Ж╪и ╪з┘Д╪е╪м╪з╪и╪з╪к ╪з┘Д╪│╪╖╪н┘К╪й."""
}

ARABIC_ENFORCEMENT = "╪к┘Ж╪и┘К┘З ╪╡╪з╪▒┘Е: ┘К╪м╪и ╪г┘Ж ┘К┘Г┘И┘Ж ╪▒╪п┘Г ╪и╪з┘Д┘Д╪║╪й ╪з┘Д╪╣╪▒╪и┘К╪й ╪з┘Д┘Б╪╡╪н┘Й (╪г┘И ╪з┘Д┘Д┘З╪м╪й ╪з┘Д╪╣╪▒╪з┘В┘К╪й ╪е╪░╪з ╪╖┘Д╪и ╪з┘Д┘Е╪│╪к╪о╪п┘Е)╪М ┘И╪н╪з┘Б╪╕ ╪╣┘Д┘Й ╪к┘Ж╪│┘К┘В RTL."

# --- ┘Е┘Ж╪╖┘В ╪з┘Д┘Е╪╣╪з┘Д╪м╪й (Core Logic) ---
async def process_chat(request: ChatRequest) -> str:
    # 1. ╪з╪о╪к┘К╪з╪▒ ╪з┘Д╪и╪▒┘И┘Е╪и╪к ╪з┘Д┘Е┘Ж╪з╪│╪и
    selected_system_prompt = SYSTEM_PROMPTS.get(request.mode, SYSTEM_PROMPTS["general"])
    full_system_message = f"{selected_system_prompt}\n\n{ARABIC_ENFORCEMENT}"

    # 2. ╪и┘Ж╪з╪б ╪│╪м┘Д ╪з┘Д╪▒╪│╪з╪ж┘Д
    messages = [SystemMessage(content=full_system_message)]
    
    # ╪е╪╢╪з┘Б╪й ╪з┘Д╪к╪з╪▒┘К╪о ╪з┘Д╪│╪з╪и┘В (Context)
    for msg in request.history:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        else:
            messages.append(AIMessage(content=msg["content"]))
            
    # ╪е╪╢╪з┘Б╪й ╪з┘Д╪▒╪│╪з┘Д╪й ╪з┘Д╪н╪з┘Д┘К╪й
    messages.append(HumanMessage(content=request.message))

    # 3. ╪з╪│╪к╪п╪╣╪з╪б ╪з┘Д┘Ж┘Е┘И╪░╪м (Invoking Groq)
    # ┘Е┘Д╪з╪н╪╕╪й: ┘З┘Ж╪з ╪│┘Ж┘В┘И┘Е ┘Д╪з╪н┘В╪з┘Л ╪и╪▒╪и╪╖ LangGraph ┘Д╪к╪┤╪║┘К┘Д ╪з┘Д╪г╪п┘И╪з╪к (Tools)
    try:
        response = await llm.ainvoke(messages)
        return response.content
    except Exception as e:
        return f"╪╣╪░╪▒╪з┘Л╪М ╪н╪п╪л ╪о╪╖╪г ╪г╪л┘Ж╪з╪б ╪з┘Д┘Е╪╣╪з┘Д╪м╪й: {str(e)}"

# --- ┘Ж┘В╪з╪╖ ╪з┘Д┘Ж┘З╪з┘К╪й (Endpoints) ---

@app.get("/")
async def root():
    return {"status": "online", "message": "┘Е╪▒╪н╪и╪з┘Л ╪и┘Г ┘Б┘К ┘Е┘Ж╪╡╪й ┘Ж╪и╪╢ 2.0 ЁЯЪА"}

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "model": "llama3-70b-8192"}

@app.post("/run", response_model=ChatResponse)
async def run_agent(request: ChatRequest):
    """
    ┘Ж┘В╪╖╪й ╪з┘Д┘Ж┘З╪з┘К╪й ╪з┘Д╪▒╪ж┘К╪│┘К╪й ┘Д┘Д┘Е╪н╪з╪п╪л╪й.
    ╪к╪│╪к┘В╪и┘Д ╪з┘Д╪▒╪│╪з┘Д╪й ┘И╪з┘Д┘И╪╢╪╣ (Mode) ┘И╪к╪╣┘К╪п ╪з┘Д╪▒╪п ╪з┘Д╪░┘Г┘К.
    """
    ai_reply = await process_chat(request)
    return ChatResponse(response=ai_reply)

# --- ╪к╪┤╪║┘К┘Д ╪з┘Д╪о╪з╪п┘Е (┘Д╪г╪║╪▒╪з╪╢ ╪з┘Д╪к╪╡╪н┘К╪н ╪з┘Д┘Е╪и╪з╪┤╪▒) ---
if __name__ == "__main__":
    uvicorn.run("app.main:app", host="0.0.0.0", port=5000, reload=True)
